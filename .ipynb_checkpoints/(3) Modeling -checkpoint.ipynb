{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 3: Modeling\n",
    "This notebook is a sequel to notebook 2 on data cleaning and EDA. This notebook seeks to execute the models and evaluate the performances using various classifiers. I will then select the best model to answer the problem statement of correct classifying a post its respective subreddit based on accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import *\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "#warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/model_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>depression</td>\n",
       "      <td>happens getting addicted pain overthinking wei...</td>\n",
       "      <td>extreme overthinking issues</td>\n",
       "      <td>no-cryptographer-68</td>\n",
       "      <td>222</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>depression</td>\n",
       "      <td>impossible get help deathwish simply forced dr...</td>\n",
       "      <td>why is there no help</td>\n",
       "      <td>delicious_sponge</td>\n",
       "      <td>850</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>depression</td>\n",
       "      <td>guess sure might enjoy sure even guess gotten ...</td>\n",
       "      <td>i wish i had managed to kill myself the first ...</td>\n",
       "      <td>yourlittlemonster</td>\n",
       "      <td>301</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>depression</td>\n",
       "      <td>sad thinking committing suicide sigh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>targetresponsible719</td>\n",
       "      <td>48</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>depression</td>\n",
       "      <td>fucking ugly disgusting big fucking forehead s...</td>\n",
       "      <td>honestly</td>\n",
       "      <td>tacowh0ree</td>\n",
       "      <td>465</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit                                               text  \\\n",
       "0  depression  happens getting addicted pain overthinking wei...   \n",
       "1  depression  impossible get help deathwish simply forced dr...   \n",
       "2  depression  guess sure might enjoy sure even guess gotten ...   \n",
       "3  depression               sad thinking committing suicide sigh   \n",
       "4  depression  fucking ugly disgusting big fucking forehead s...   \n",
       "\n",
       "                                               title                author  \\\n",
       "0                        extreme overthinking issues   no-cryptographer-68   \n",
       "1                               why is there no help      delicious_sponge   \n",
       "2  i wish i had managed to kill myself the first ...     yourlittlemonster   \n",
       "3                                                NaN  targetresponsible719   \n",
       "4                                           honestly            tacowh0ree   \n",
       "\n",
       "   text_length  word_count  \n",
       "0          222          45  \n",
       "1          850         171  \n",
       "2          301          61  \n",
       "3           48           9  \n",
       "4          465          98  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit      0\n",
       "text           5\n",
       "title          5\n",
       "author         0\n",
       "text_length    0\n",
       "word_count     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3734, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create custom stopwords list to add to default stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subreddit'] = df['subreddit'].map({'depression': int(0), 'foreveralone': int(1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = ['im', 'like', 'wa', 'dont','feel', 'life', 'time', 'ive', 'really', 'think', 'make', \n",
    "                    'people', 'year', 'thing', 'friend', 'day', 'ha', 'way', 'got', 'know', 'want', 'good', 'say',\n",
    "                    'word', 'going', 'didnt', 'work', 'thought', 'talk', 'love', 'feeling', 'thats', 'said']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop.extend(custom_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Pass: Initial Models with Default Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first iteration of my modeling process, i am going to use default parameters to run the 4 different classifiers - Logistic Regression, Random Forest, K Nearest Neighbours, Multinomial Naive Bayes. I will do an evaluation after the process.\n",
    "\n",
    "We will start off with defining our X and Y, doing a train test split along with identifying the baseline accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X & Y\n",
    "X = df['text']\n",
    "y = df['subreddit']\n",
    "\n",
    "# Split the data into the training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.510712\n",
       "1    0.489288\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline Accuracy\n",
    "y.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tvec', TfidfVectorizer()), ('lr', LogisticRegression())])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Pipeline\n",
    "pipe_lr = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('lr', LogisticRegression())])\n",
    "\n",
    "pipe_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Score: 0.9312275089964014\n",
      "Logistic Regression Accuracy Score: 0.8248175182481752\n",
      "Logistic Regression Variance: 0.10640999074822621\n",
      "Logistic Regression Cross Validation Score: 0.8275342657493244\n",
      "Logistic Regression ROC_AUC Score: 0.9089323355817875\n"
     ]
    }
   ],
   "source": [
    "# Generating scores\n",
    "train_score_lr = pipe_lr.score(X_train, y_train)\n",
    "print(f'Logistic Regression Train Score:', train_score_lr)\n",
    "pred_lr = pipe_lr.predict(X_test)\n",
    "accuracy_lr =  accuracy_score(pred_lr, y_test)\n",
    "print(f'Logistic Regression Accuracy Score:', accuracy_lr)\n",
    "var_lr = train_score_lr - accuracy_lr\n",
    "print(f'Logistic Regression Variance:', var_lr)\n",
    "crossval_lr = cross_val_score(pipe_lr, X, y, cv=5)\n",
    "print(f'Logistic Regression Cross Validation Score:', crossval_lr.mean())\n",
    "preds_lr = pipe_lr.predict_proba(X_test)[:,1]\n",
    "roc_lr = roc_auc_score(y_test, preds_lr)\n",
    "print(f'Logistic Regression ROC_AUC Score:', roc_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine X & Y\n",
    "X = df['text']\n",
    "y = df['subreddit']\n",
    "\n",
    "# Split the data into the training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                ('rf', RandomForestClassifier(random_state=42))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Pipeline\n",
    "pipe_rf = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "pipe_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Train Score: 1.0\n",
      "Random Forest Accuracy Score: 0.7996755879967559\n",
      "Random Forest Variance: 0.20032441200324413\n",
      "Random Forest Cross Validation Score: 0.8020905785788373\n",
      "Random Forest ROC_AUC Score: 0.8843209844013491\n"
     ]
    }
   ],
   "source": [
    "train_score_rf = pipe_rf.score(X_train, y_train)\n",
    "print(f'Random Forest Train Score:', train_score_rf)\n",
    "pred_rf = pipe_rf.predict(X_test)\n",
    "accuracy_rf =  accuracy_score(pred_rf, y_test)\n",
    "print(f'Random Forest Accuracy Score:', accuracy_rf)\n",
    "var_rf = train_score_rf - accuracy_rf\n",
    "print(f'Random Forest Variance:', var_rf)\n",
    "crossval_rf = cross_val_score(pipe_rf, X, y, cv=5)\n",
    "print(f'Random Forest Cross Validation Score:', crossval_rf.mean())\n",
    "preds_rf = pipe_rf.predict_proba(X_test)[:,1]\n",
    "roc_rf = roc_auc_score(y_test, preds_rf)\n",
    "print(f'Random Forest ROC_AUC Score:', roc_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine X & Y\n",
    "X = df['text']\n",
    "y = df['subreddit']\n",
    "\n",
    "# Split the data into the training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                ('knn', KNeighborsClassifier(n_neighbors=2))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Pipeline\n",
    "pipe_knn = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=2))])\n",
    "\n",
    "pipe_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbours Train Score: 0.8428628548580568\n",
      "K Nearest Neighbours Accuracy Score: 0.683698296836983\n",
      "K Nearest Neighbours Variance: 0.15916455802107377\n",
      "K Nearest Neighbours Cross Validation Score: 0.671132429629151\n",
      "K Nearest Neighbours ROC_AUC Score: 0.748477023608769\n"
     ]
    }
   ],
   "source": [
    "train_score_knn = pipe_knn.score(X_train, y_train)\n",
    "print(f'K Nearest Neighbours Train Score:', train_score_knn)\n",
    "pred_knn = pipe_knn.predict(X_test)\n",
    "accuracy_knn =  accuracy_score(pred_knn, y_test)\n",
    "print(f'K Nearest Neighbours Accuracy Score:', accuracy_knn)\n",
    "var_knn = train_score_knn - accuracy_knn\n",
    "print(f'K Nearest Neighbours Variance:', var_knn)\n",
    "crossval_knn = cross_val_score(pipe_knn, X, y, cv=5)\n",
    "print(f'K Nearest Neighbours Cross Validation Score:', crossval_knn.mean())\n",
    "preds_knn = pipe_knn.predict_proba(X_test)[:,1]\n",
    "roc_knn = roc_auc_score(y_test, preds_knn)\n",
    "print(f'K Nearest Neighbours ROC_AUC Score:', roc_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine X & Y\n",
    "X = df['text']\n",
    "y = df['subreddit']\n",
    "\n",
    "# Split the data into the training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tvec', TfidfVectorizer()), ('mnb', MultinomialNB())])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Pipeline\n",
    "pipe_mnb = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('mnb', MultinomialNB())])\n",
    "\n",
    "pipe_mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Train Score: 0.9196321471411435\n",
      "Multinomial Naive Bayes Accuracy Score: 0.8215733982157339\n",
      "Multinomial Naive Bayes Variance: 0.09805874892540956\n",
      "Multinomial Naive Bayes Cross Validation Score: 0.8146778355602932\n",
      "Multinomial Naive Bayes ROC_AUC Score: 0.9090930649241147\n"
     ]
    }
   ],
   "source": [
    "train_score_mnb = pipe_mnb.score(X_train, y_train)\n",
    "print(f'Multinomial Naive Bayes Train Score:', train_score_mnb)\n",
    "pred_mnb = pipe_mnb.predict(X_test)\n",
    "accuracy_mnb =  accuracy_score(pred_mnb, y_test)\n",
    "print(f'Multinomial Naive Bayes Accuracy Score:', accuracy_mnb)\n",
    "var_mnb = train_score_mnb - accuracy_mnb\n",
    "print(f'Multinomial Naive Bayes Variance:', var_mnb)\n",
    "crossval_mnb = cross_val_score(pipe_mnb, X, y, cv=5)\n",
    "print(f'Multinomial Naive Bayes Cross Validation Score:', crossval_mnb.mean())\n",
    "preds_mnb = pipe_mnb.predict_proba(X_test)[:,1]\n",
    "roc_mnb = roc_auc_score(y_test, preds_mnb)\n",
    "print(f'Multinomial Naive Bayes ROC_AUC Score:', roc_mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of First Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>K Nearest Neighbours</th>\n",
       "      <th>Multinomial Naive Bayes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Score</th>\n",
       "      <td>0.931228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.842863</td>\n",
       "      <td>0.919632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy Score</th>\n",
       "      <td>0.824818</td>\n",
       "      <td>0.799676</td>\n",
       "      <td>0.683698</td>\n",
       "      <td>0.821573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.106410</td>\n",
       "      <td>0.200324</td>\n",
       "      <td>0.159165</td>\n",
       "      <td>0.098059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cross Val</th>\n",
       "      <td>0.827534</td>\n",
       "      <td>0.802091</td>\n",
       "      <td>0.671132</td>\n",
       "      <td>0.814678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC AUC Score</th>\n",
       "      <td>0.908932</td>\n",
       "      <td>0.884321</td>\n",
       "      <td>0.748477</td>\n",
       "      <td>0.909093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Logistic Regression  Random Forest  K Nearest Neighbours  \\\n",
       "Train Score                0.931228       1.000000              0.842863   \n",
       "Accuracy Score             0.824818       0.799676              0.683698   \n",
       "Variance                   0.106410       0.200324              0.159165   \n",
       "Cross Val                  0.827534       0.802091              0.671132   \n",
       "ROC AUC Score              0.908932       0.884321              0.748477   \n",
       "\n",
       "                Multinomial Naive Bayes  \n",
       "Train Score                    0.919632  \n",
       "Accuracy Score                 0.821573  \n",
       "Variance                       0.098059  \n",
       "Cross Val                      0.814678  \n",
       "ROC AUC Score                  0.909093  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a df to compare results between models\n",
    "first_pass_df = pd.DataFrame([[train_score_lr, train_score_rf, train_score_knn, train_score_mnb],\n",
    "                              [accuracy_lr, accuracy_rf, accuracy_knn, accuracy_mnb],\n",
    "                              [var_lr, var_rf, var_knn, var_mnb],\n",
    "                              [crossval_lr.mean(), crossval_rf.mean(), crossval_knn.mean(), crossval_mnb.mean()],\n",
    "                              [roc_lr, roc_rf, roc_knn, roc_mnb]], \n",
    "                              columns = [\"Logistic Regression\", \"Random Forest\", \"K Nearest Neighbours\", \"Multinomial Naive Bayes\"], \n",
    "                              index=[\"Train Score\", \"Accuracy Score\", \"Variance\", \"Cross Val\", \"ROC AUC Score\"])\n",
    "first_pass_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Pass Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under this iteration, both the Logistic Regression, Random Forest and Multinomial Naive Bayes model had the similar scores, minus the very high train score for Random Forest which coincidences with the highest variance. The K Nearest Neighbours model, however, did very poorly with the lowest scores in almost all score types.\n",
    "\n",
    "Next we are going to try tuning the models to find the best hyperparameters and select the best model from there for our final model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Pass: Models with GridSearch Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, i am going to tune the hyper paramaters to spice things up and identify the best paramaters using grid search and custom hyperparameters for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine X & Y\n",
    "X = df['text']\n",
    "y = df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Pipeline and GridSearch Function to Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code adapted from James Ellis\n",
    "\n",
    "def lr_pipe_func(pipe, params):\n",
    "    ''' Function to execute Gridsearch CV to find \n",
    "        best parameters for Logistic Regression Model \n",
    "        Output returns the relevant scores'''\n",
    "    gs_lr2 = GridSearchCV(estimator=pipe, param_grid=params, cv=5, verbose=1, n_jobs=6)\n",
    "    gs_lr2.fit(X_train, y_train)\n",
    "    train_score_lr2 = gs_lr2.score(X_train, y_train)\n",
    "    print(f'Training Score:', train_score_lr2)\n",
    "    y_pred_lr2 = gs_lr2.best_estimator_.predict(X_test)\n",
    "    accuracy_lr2 = accuracy_score(y_pred_lr2, y_test)\n",
    "    print(f\"Accuracy Score:\", accuracy_lr2)\n",
    "    var_lr2 = train_score_lr2 - accuracy_lr2\n",
    "    print(f\"Variance:\", var_lr2)\n",
    "    crossval_lr2 = gs_lr2.best_score_\n",
    "    print(f'CrossVal Score:', crossval_lr2)\n",
    "    y_pred_prob_lr2 = gs_lr2.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "    roc_lr2 = roc_auc_score(y_test, y_pred_prob_lr2)\n",
    "    print(f\"ROC_AUC Score:\", roc_lr2)\n",
    "    print(f'Best Params:{gs_lr2.best_params_}')\n",
    "    return gs_lr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=6)]: Done 188 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=6)]: Done 438 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=6)]: Done 720 out of 720 | elapsed:   23.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9312275089964014\n",
      "Accuracy Score: 0.8248175182481752\n",
      "Variance: 0.10640999074822621\n",
      "CrossVal Score: 0.8268686626746508\n",
      "ROC_AUC Score: 0.9089323355817875\n",
      "Best Params:{'logreg__C': 1, 'logreg__penalty': 'l2', 'tvec__max_df': 0.5, 'tvec__max_features': None, 'tvec__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "lr_pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    (\"logreg\", LogisticRegression())\n",
    "])\n",
    "\n",
    "lr_params = {\n",
    "    'tvec__max_df': (0.5, 0.75, 1.0),\n",
    "    'tvec__max_features': (None, 1000, 2000, 3000),\n",
    "    'tvec__ngram_range': ((1, 1),),\n",
    "    \"logreg__penalty\": [\"l1\", \"l2\"],\n",
    "    \"logreg__C\": [0.001,0.01,0.1,1,10,100]\n",
    "}\n",
    "\n",
    "# running the gridsearch on the whole thing\n",
    "lr_model = lr_pipe_func(lr_pipe, lr_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_pipe_func(pipe, params):\n",
    "    ''' Function to execute Gridsearch CV to find\n",
    "        best parameters for Random Forest model.\n",
    "        Output returns the relevant scores'''\n",
    "    gs_rf2 = GridSearchCV(estimator=pipe, param_grid=params, cv=5, verbose=1, n_jobs=6)\n",
    "    gs_rf2.fit(X_train, y_train)\n",
    "    train_score_rf2 = gs_rf2.score(X_train, y_train)\n",
    "    print(f'Training Score:', train_score_rf2)\n",
    "    y_pred_rf2 = gs_rf2.best_estimator_.predict(X_test)\n",
    "    accuracy_rf2 = accuracy_score(y_pred_rf2, y_test)\n",
    "    print(f\"Accuracy Score:\", accuracy_rf2)\n",
    "    var_rf2 = train_score_rf2 - accuracy_rf2\n",
    "    print(f\"Variance:\", var_rf2)\n",
    "    crossval_rf2 = gs_rf2.best_score_\n",
    "    print(f'CrossVal Score:', crossval_rf2)\n",
    "    y_pred_prob_rf2 = gs_rf2.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "    roc_rf2 = roc_auc_score(y_test, y_pred_prob_rf2)\n",
    "    print(f\"ROC_AUC Score:\", roc_rf2)\n",
    "    print(f'Best Params:{gs_rf2.best_params_}')\n",
    "    return gs_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 648 candidates, totalling 3240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=6)]: Done 188 tasks      | elapsed:   31.4s\n",
      "[Parallel(n_jobs=6)]: Done 438 tasks      | elapsed:  1.0min\n"
     ]
    }
   ],
   "source": [
    "rf_pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    (\"rand_for\", RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "rf_params = {\n",
    "    'tvec__max_df': (0.5, 0.75, 1.0),\n",
    "    'tvec__max_features': (None, 1000, 2000, 3000),\n",
    "    'tvec__ngram_range': ((1, 1),),\n",
    "    \"rand_for__n_estimators\": [50, 100, 150],\n",
    "    \"rand_for__max_depth\": [100, 200, 300],\n",
    "    \"rand_for__min_samples_split\": [5, 25, 50],\n",
    "    \"rand_for__criterion\": ['gini', 'entropy'],\n",
    "}\n",
    "\n",
    "# running the gridsearch on the whole thing\n",
    "rf_model = rf_pipe_func(rf_pipe, rf_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_pipe_func(pipe, params):\n",
    "    ''' Function to execute Gridsearch CV to find\n",
    "        best parameters for K Nearest Neighbours model. \n",
    "        Output returns the relevant scores'''\n",
    "    gs_knn2 = GridSearchCV(estimator=pipe, param_grid=params, cv=5, verbose=1, n_jobs=6)\n",
    "    gs_knn2.fit(X_train, y_train)\n",
    "    train_score_knn2 = gs_knn2.score(X_train, y_train)\n",
    "    print(f'Training Score:', train_score_knn2)\n",
    "    y_pred_knn2 = gs_knn2.best_estimator_.predict(X_test)\n",
    "    accuracy_knn2 = accuracy_score(y_pred_knn2, y_test)\n",
    "    print(f\"Accuracy Score:\", accuracy_knn2)\n",
    "    var_knn2 = train_score_knn2 - accuracy_knn2\n",
    "    print(f\"Variance:\", var_knn2)\n",
    "    crossval_knn2 = gs_knn2.best_score_\n",
    "    print(f'CrossVal Score:', crossval_knn2)\n",
    "    y_pred_prob_knn2 = gs_knn2.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "    roc_knn2 = roc_auc_score(y_test, y_pred_prob_knn2)\n",
    "    print(f\"ROC_AUC Score:\", roc_knn2)\n",
    "    print(f'Best Params:{gs_knn2.best_params_}')\n",
    "    return gs_knn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    (\"knn\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "knn_params = {\n",
    "    'tvec__max_df': (0.5, 0.75, 1.0),\n",
    "    'tvec__max_features': (None, 1000, 2000, 3000),\n",
    "    'tvec__ngram_range': ((1, 1),),\n",
    "    \"knn__n_neighbors\": [75, 85, 95]\n",
    "}\n",
    "\n",
    "# running the gridsearch on the whole thing\n",
    "knn_model = knn_pipe_func(knn_pipe, knn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnb_pipe_func(pipe, params):\n",
    "    ''' Function to execute Gridsearch CV to find\n",
    "        best parameters for Multinomial Naive Bayes model. \n",
    "        Output returns the relevant scores'''\n",
    "    gs_mnb2 = GridSearchCV(estimator=pipe, param_grid=params, cv=5, verbose=1, n_jobs=6)\n",
    "    gs_mnb2.fit(X_train, y_train)\n",
    "    train_score_mnb2 = gs_mnb2.score(X_train, y_train)\n",
    "    print(f'Training Score:', train_score_mnb2)\n",
    "    y_pred_mnb2 = gs_mnb2.best_estimator_.predict(X_test)\n",
    "    accuracy_mnb2 = accuracy_score(y_pred_mnb2, y_test)\n",
    "    print(f\"Accuracy Score:\", accuracy_mnb2)\n",
    "    var_mnb2 = train_score_mnb2 - accuracy_mnb2\n",
    "    print(f\"Variance:\", var_mnb2)\n",
    "    crossval_mnb2 = gs_mnb2.best_score_\n",
    "    print(f'CrossVal Score:', crossval_mnb2)\n",
    "    y_pred_prob_mnb2 = gs_mnb2.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "    roc_mnb2 = roc_auc_score(y_test, y_pred_prob_mnb2)\n",
    "    print(f\"ROC_AUC Score:\", roc_mnb2)\n",
    "    print(f'Best Params:{gs_mnb2.best_params_}')\n",
    "    return gs_mnb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    (\"mnb\", MultinomialNB())\n",
    "])\n",
    "\n",
    "mnb_params = {\n",
    "    'tvec__max_df': (0.5, 0.75, 1.0),\n",
    "    'tvec__max_features': (None, 1000, 2000, 3000),\n",
    "    'tvec__ngram_range': ((1, 1),),\n",
    "    \"mnb__alpha\": [0.125, 0.15, 0.175]\n",
    "}\n",
    "\n",
    "# running the gridsearch on the whole thing\n",
    "mnb_model = mnb_pipe_func(mnb_pipe, mnb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Second Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new df\n",
    "second_model_df = pd.DataFrame(index=[\"Logistic Regression\", \"Random Forest\", \"K Nearest Neighbours\", \"Multinomial Naive Bayes\"])\n",
    "                               \n",
    "models = [lr_model, rf_model, knn_model, mnb_model]\n",
    "\n",
    "# putting in the columns\n",
    "second_model_df[\"Train Score\"] = [round(model.score(X_train, y_train), 4) for model in models]\n",
    "second_model_df[\"Accuracy\"] = [round(model.score(X_test, y_test), 4) for model in models]\n",
    "second_model_df[\"Variance\"] = second_model_df[\"Train Score\"] - second_model_df[\"Accuracy\"]\n",
    "second_model_df[\"Cross Val\"] = [round(model.best_score_, 4) for model in models]\n",
    "\n",
    "second_model_df                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pass_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second iteration, we see a slight decrease across the models in train scores except for Logistic Regression model. Accuracy score remains consistent with K Nearest Neighbours model having a huge increase from 0.6836 in the first model to 0.8029 in the second model. Variance also remains rather consistent except for K Nearest Neighbours model having an increase which probably correlated to the improvement in accuracy score as the model tries to fit better. This also results in an improvement in cross validation scores for K Nearest Neighbours while the other models remains constant. \n",
    "\n",
    "Overall, all the Logistic Regression and Multinomial Naive Bayes model have relatively close scores. For my final model, i have decided to go with the Multinomial Naive Bayes model due to it having the lowest variance and highest accuracy score. Hence this model does not models the training data too well and can provide a more accurate prediction as it can better generalise on new data. It has a significant improvement (0.8289) compared to the baseline model (0.5107) in terms of accuracy score, topping it by about 32%. On the other hand, the Logistic Regression model has a relatively higher training score compared to the Multinomial Naive Bayes model, suggesting a lower bias, but it is countered by a higher variance.\n",
    "\n",
    "As our main goal is to correctly predict which subreddit a post belongs in, accuracy plays a big part in model selection. It is important to ensure a post is correctly classified depression or forever alone in line with our problem statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model\n",
    "### Multinomial Naive Bayes Model Fitted with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine X & Y\n",
    "X = df['text']\n",
    "y = df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pipeline\n",
    "final_model_pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(max_df=0.5, max_features=1000, ngram_range=(1, 1))),\n",
    "    ('mnb', MultinomialNB(alpha=0.125))])\n",
    "\n",
    "final_model = final_model_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather final model results\n",
    "train_final_model = final_model.score(X_train, y_train)\n",
    "print(f'Final Model Training Score:', train_final_model)\n",
    "\n",
    "accuracy_final_model = final_model.score(X_test, y_test)\n",
    "print(f\"Final Model Accuracy Score:\", accuracy_final_model)\n",
    "\n",
    "var_final_model = train_final_model - accuracy_final_model\n",
    "print(f\"Final Model Variance:\", var_final_model)\n",
    "\n",
    "crossval_final_model = cross_val_score(final_model, X_train, y_train, cv=5).mean()\n",
    "print(f\"Final Model Cross Validation Score:\", crossval_final_model)\n",
    "\n",
    "y_pred_final_model = final_model.predict(X_test)\n",
    "roc_final_model = roc_auc_score(y_test, y_pred_final_model)\n",
    "print(f\"Final Model ROC AUC Score:\", roc_final_model)\n",
    "\n",
    "final_model_df = pd.DataFrame(index=[\"Multinomial Naive Bayes Final Model\"])\n",
    "final_model_df[\"Train Score\"] = [round(train_final_model, 4)]\n",
    "final_model_df[\"Accuracy\"] = [round(accuracy_final_model, 4)]\n",
    "final_model_df[\"Variance\"] = final_model_df[\"Train Score\"] - final_model_df[\"Accuracy\"]\n",
    "final_model_df[\"Cross Val Score\"] = [round(crossval_final_model, 4)]\n",
    "final_model_df[\"ROC_AUC Score\"] = [round(roc_final_model, 4)]\n",
    "\n",
    "final_model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristrics (ROC) Curve\n",
    "Another way to justify my reason for selecting the Multinomial Naive Bayes model is by visualizing the ROC Curve and calculating the Area Under Curve (AUC). This curve plots the true positive rate against the false positive rate to show us how the model makes decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc(probas, true, step=0.01):\n",
    "    \"\"\"\n",
    "    probas should be a numpy array of predict_probas\n",
    "    true is a pandas series of true labels\n",
    "    step is the step size for checking thresholds\n",
    "    \"\"\"\n",
    "    \n",
    "    probas = probas[:,1]  # The output of predict_proba() is an array of the probabilities for every class, but we only want the probabilities for class 1\n",
    "    true = true.values    # We need to convert the class labels from a Pandas Series to a numpy array. We do this using the .values attribute\n",
    "    assert(len(probas) == len(true)) # We're making sure that our probabilities vector is the same length as our true class labesl vector\n",
    "    \n",
    "    TPRs = [] # Setting up empty list of True Positive Rate\n",
    "    FPRs = [] # Setting up empty list of False Positive Rate\n",
    "    \n",
    "    for i in np.arange(0.0,1.0,step): # np.arange allows us to use step sizes that are decimals\n",
    "        preds_class = probas > i # Numpy arrays have a feature called 'broadcasting.' Check the documentation: https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html to see what this does.\n",
    "        TP = 0 \n",
    "        FP = 0\n",
    "        TN = 0\n",
    "        FN = 0\n",
    "        for index in range(len(preds_class)): # We're comparing each prediction with each true value here\n",
    "\n",
    "            if preds_class[index] == 1 and true[index] == 1:\n",
    "                TP += 1\n",
    "            elif preds_class[index] == 1 and true[index] == 0:\n",
    "                FP += 1\n",
    "            elif preds_class[index] == 0 and true[index] == 0:\n",
    "                TN += 1 \n",
    "            elif preds_class[index] == 0 and true[index] == 1:\n",
    "                FN += 1\n",
    "                \n",
    "        TPR = TP/(TP + FN) # Calculating TPR and FPR and appending to our lists\n",
    "        FPR = FP/(FP + TN)\n",
    "        \n",
    "        TPRs.append(TPR)\n",
    "        FPRs.append(FPR)\n",
    "         \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.rcParams['font.size'] = 14\n",
    "    plt.plot(FPRs, TPRs, color=\"orange\", label='ROC Curve')\n",
    "    plt.plot([0,1],[0,1], color='navy', lw=2, linestyle='--', label='Baseline')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "    plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "    plt.legend()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = final_model.predict_proba(X_test)\n",
    "\n",
    "roc(probas = probabilities, # pass in series of probabilities \n",
    "    true = y_test,          # pass in series of true values\n",
    "    step=0.001); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Model ROC AUC Score: 0.8283\n",
    "    \n",
    "The ROC curve has a decently smooth curve, with the apex of the curve appearing to between a sensitivity range of 0.7 to 0.9. Hence, this apex would be the best point of probability to classify whether a post belong to the depression or forever alone subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model. This gives us a holistic view of how well our classification model is performing and what kinds of errors it is making. (Analytics Vidhya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred_final_model = final_model.predict(X_test)\n",
    "\n",
    "# Save confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_final_model).ravel()\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plot_confusion_matrix(final_model, X_test, y_test, cmap='Blues', values_format='d');\n",
    "\n",
    "# Calculate Accuracy using confusion matrix\n",
    "acc = (tp + tn) / (tp + tn + fp + fn) \n",
    "\n",
    "# Calculate the True Negative Rate / Specificity\n",
    "spec = tn / (tn + fp)\n",
    "\n",
    "# True Positive Rate / Sensitivity\n",
    "sen = tp / (tp + fn)\n",
    "\n",
    "# Precision\n",
    "prec = tp / (tp + fp)\n",
    "\n",
    "print('Accuracy:' , acc)\n",
    "print('Specificity:', spec)\n",
    "print('Sensitivity:', sen)\n",
    "print('Precision:', prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the confusion matrix, a False Positive (FP) value of 100, means 100 posts in the Forever Alone subreddit were incorrectly classified as belonging to the Depression subreddit. On the other hand, a False Negative (FN) value of 111 means 111 posts in Depression subreddit were incorrectly classified as belonging to the Forever Alone subreddit. 540 Depression subreddit posts were correctly classfied and 482 Forever Alone posts were correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion, Summary and Recommendations\n",
    "\n",
    "This project about binary classification problem has been an interesting one yet tricky one. The goal of the project is to use Natural Language Processing (NLP) to train a classifier on which subreddit a given post came from. In this case it would be to come up with a model that can best effecfively classify if a post in a subreddit is from the Depression or Forever Alone subreddit based on the post text.\n",
    "\n",
    "Through several testing and iterations, i have chosen to go with the Multinomial Naive Bayes model based on test results in comparison with the other models like Logistic Regression, K Nearest Neighbours and Random Forest. With consistently higher scores and low variance, the model manage to produce a 83% accuracy which is 33% higher of the baseline accuracy of 51%.\n",
    "\n",
    "This results may not be the best but the model could be pretty useful and applicable in real situations. It could be a key factor in early detection of depression based on (1) correct classification of a post in the Depression subreddit and (2) words used by a user in his/her post.\n",
    "\n",
    "In addition, during the exploratory data analysis segment, i figured out the top unique words in the top 50 common words of both subreddit by removing high frequency common words in both dataframes. These words suggest that a post belong to a particular subreddit. However, some of those words like better, bad, need are pretty generic and does not tells much about the full story.\n",
    "\n",
    "Hence, i further zoomed in on the words that are totally unique (not just unique in the top 50 most common words) that classifies a person posting in the depression subreddit. The top 10 words include: med, diagnosed, antidepressant, psychiatrist, depressive, disappear, disorder, cutting, mum, treatment.\n",
    "\n",
    "These more powerful words clearer show the factors affecting the potentially depressed user. By finding these unique key words in the depression subreddits, one may be able to identify a potentially depressed user. Along with applying the model, it works towards the problem statement of classifying the correct subreddit.\n",
    "\n",
    "With regards to future work, given more time, i would like to optimise the model. I ought to explore the effects on other features like word count/length and title words to see how i can come up with a better and more accurate model to classify a post. This features could be useful in affecting the mix in the model as the initial model was purely using the post texts as a variable along with some feature engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depression subreddit extracted from: https://www.reddit.com/r/depression/\n",
    "\n",
    "Forever Alone subreddit extracted from: https://www.reddit.com/r/ForeverAlone/comments/muj5p8/i_know_i_will_be_forever_alone/\n",
    "\n",
    "Confusion Matrix extracted from: https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
