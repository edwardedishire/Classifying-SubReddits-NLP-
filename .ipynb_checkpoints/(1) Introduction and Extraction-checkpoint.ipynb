{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Binary Classification of subreddits\n",
    "# Notebook 1: Introduction and Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Mental health issues have been on a high in recent times, with increasing cases of as a result of factors like post traumatic stress disorder (PTSD), bullying (real-life or cyber) and suicides. All these lead to depression which is a cause of concern for the government. \n",
    "\n",
    "Being part of a research team in the Institute of Mental Health (IMH), my team is tasked to come up a solution that can accurately predict and classify a post on reddit according to the subreddits (depression and forever alone). Both topics are commonly thought to be similar but they are not. Forever alone is a phrase or term used by a single person to express his loneliness at not having a significant other; or in broader perspective, lack of friends. It is also used more humorously at times as a slang or internet meme. This term is linked to people claiming to be \"depressed\" but in fact they are not.\n",
    "\n",
    "On the other hand, depression is a serious matter and more often than not, one do not even realize that they actually have depression. It is also extremely challenging to determine if a person has depression by relying on behaviorial cues, even for a highly qualified psychiatrist. Furthermore, if the depressed person is introverted, he/she might not want to share his problems openly to his/her family or friends or even seek professional help in fear being stigmatized by society. As such, the person might turn to reddit, an online forum, to express his/her personal feelings and thoughts with other users in the same subreddit annoymously. He/she might feel more comfortable in that way. That being said, a user with depression might get confused over the terms depression and forever alone in thinking they are the same , thus posting in the forever alone subreddit rather than the depression subreddit. \n",
    "\n",
    "Hence, we aim to come up with a model that is able to accurately classify a post to the respective subreddits (depression/forever alone). Our model also aims to check for misclassification of post by diving deeper into the post, unraveling the false positive and identifying top 10 words of each subreddit. Hence, we are able to provide early detection of potential depression cases along with identify existing cases. We can then look to get them the help they need. The target audience of this project is everyone, especially those working in the psychological and healthcare departments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "The objective of this project is to execute different binary classification algorithms/models and find the best model over several attempts. Firstly, i will extract about 2000 posts for the depression and forever alone subreddits. I will then clean the data to remove duplicates, missing values and outliers. \n",
    "\n",
    "Next up is the exploratory data analysis (EDA) where i analyze the data, identifying relationships  and displaying their distrbution.  This would give a quick view of what are the deciding variables in relation to classifying the respective subreddits.\n",
    "\n",
    "It is then followed by pre-processing, where i began to prepare my data for the modeling process. This step includes lemmatizing my words to the base root, removing default english stopwords and my own customised stopwords which consists of top common words that exist in both datasets.\n",
    "\n",
    "As for modeling, my initial models will be without any adjustments to hyperparameters. My second attempt will be to run the models with tuned hyperparamaters and find the most accurate model. Lastly, i will use my final model and evaluate it further to conclude.\n",
    "\n",
    "I will be using count and tfidf vectorizers to tokenise the variable columns. I decided to make use of 4 different classifiers (Logistic Regression, Random Forest, K Nearest Neighbours and Multinomial Naive Bayes) to create different variations. I will then evaluate their respective scores along with the confusion matrix to evaluate and select my final model.\n",
    "\n",
    "The final model selected was the Multinimial Naive Bayes model equipped with the tfidf vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, i will go on to pull posts from the 2 subreddits using PushShift API and store them as dataframes. I will create a function to execute the extraction. Pushshift API limits me to 100 posts per pull. I have included a sleep timer to avoid getting blocked by reddit for repeated pulling in short intervals. \n",
    "\n",
    "The function also include the number of times the pull is executed and the cumulative number of posts pulled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was adapted and referenced from my groupmates Mark and Chin Xia who guided me on the process\n",
    "\n",
    "def pull_reddit(subreddit):\n",
    "    ''' Function to pull posts from subreddits.\n",
    "        Output returns the post pulled. '''\n",
    "    df = pd.DataFrame()\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    bef_counter = '0d'\n",
    "    i = 0\n",
    "    while 2000 - len(df) > 0:\n",
    "        time.sleep(np.random.randint(1,5)) # Setting a sleep timer\n",
    "        params = {\n",
    "            'subreddit': subreddit,\n",
    "            'size': 100,\n",
    "            'before': bef_counter,\n",
    "            'unique': 'selftext'}\n",
    "        res = requests.get(url, params)\n",
    "        print(f'Pull Number {i}, Status Code: {res.status_code}')\n",
    "        if res.status_code == 200: # Status_code check\n",
    "            data = res.json()\n",
    "            new_df = pd.DataFrame(data['data'])\n",
    "            cleaned_df = new_df.drop(new_df[\n",
    "                (new_df['selftext'] == '[removed]') | # Dropping removed posts\n",
    "                (new_df['selftext'] == '[deleted]') | # Dropping deleted posts\n",
    "                (new_df['selftext'] == '') | # Dropping empty posts\n",
    "                (new_df['is_video'] == True) # Dropping non-text posts\n",
    "            ].index)\n",
    "            bef_counter = str(i*10)+'d'\n",
    "            df = pd.concat([df, cleaned_df], axis = 0)\n",
    "            i += 1\n",
    "            print(f'Number of post pulled : {len(df)}')\n",
    "        else:\n",
    "            print(f'Loop {i} failed, pausing script')\n",
    "            time.sleep(np.random.randint(2,3))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pull Number 0, Status Code: 200\n",
      "Number of post pulled : 87\n",
      "Pull Number 1, Status Code: 200\n",
      "Number of post pulled : 174\n",
      "Pull Number 2, Status Code: 200\n",
      "Number of post pulled : 256\n",
      "Pull Number 3, Status Code: 200\n",
      "Number of post pulled : 348\n",
      "Pull Number 4, Status Code: 200\n",
      "Number of post pulled : 438\n",
      "Pull Number 5, Status Code: 200\n",
      "Number of post pulled : 519\n",
      "Pull Number 6, Status Code: 200\n",
      "Number of post pulled : 604\n",
      "Pull Number 7, Status Code: 200\n",
      "Number of post pulled : 690\n",
      "Pull Number 8, Status Code: 200\n",
      "Number of post pulled : 768\n",
      "Pull Number 9, Status Code: 200\n",
      "Number of post pulled : 850\n",
      "Pull Number 10, Status Code: 200\n",
      "Number of post pulled : 936\n",
      "Pull Number 11, Status Code: 200\n",
      "Number of post pulled : 1016\n",
      "Pull Number 12, Status Code: 200\n",
      "Number of post pulled : 1104\n",
      "Pull Number 13, Status Code: 200\n",
      "Number of post pulled : 1186\n",
      "Pull Number 14, Status Code: 200\n",
      "Number of post pulled : 1267\n",
      "Pull Number 15, Status Code: 200\n",
      "Number of post pulled : 1353\n",
      "Pull Number 16, Status Code: 200\n",
      "Number of post pulled : 1442\n",
      "Pull Number 17, Status Code: 200\n",
      "Number of post pulled : 1532\n",
      "Pull Number 18, Status Code: 200\n",
      "Number of post pulled : 1615\n",
      "Pull Number 19, Status Code: 200\n",
      "Number of post pulled : 1699\n",
      "Pull Number 20, Status Code: 200\n",
      "Number of post pulled : 1787\n",
      "Pull Number 21, Status Code: 200\n",
      "Number of post pulled : 1876\n",
      "Pull Number 22, Status Code: 200\n",
      "Number of post pulled : 1963\n",
      "Pull Number 23, Status Code: 200\n",
      "Number of post pulled : 2039\n"
     ]
    }
   ],
   "source": [
    "depression = pull_reddit('depression')\n",
    "depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forever_alone = pull_reddit('ForeverAlone')\n",
    "forever_alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(depression.shape)\n",
    "print(forever_alone.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depression = depression[['subreddit', 'selftext','title', 'author']]\n",
    "depression = pd.DataFrame(depression)\n",
    "depression.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forever_alone = forever_alone[['subreddit', 'selftext','title', 'author']]\n",
    "forever_alone = pd.DataFrame(forever_alone)\n",
    "forever_alone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#depression.to_csv('./data/depression.csv', index=False)\n",
    "#forever_alone.to_csv('./data/forever_alone.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
